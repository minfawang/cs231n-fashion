{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minfa/.local/share/virtualenvs/cs231n-fashion-rD9JfZw-/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 2\n",
    "batch_size = 2\n",
    "num_parallel_batches = 2\n",
    "\n",
    "def map_fn(image, label):\n",
    "    return image, label\n",
    "    \n",
    "\n",
    "def input_fn():\n",
    "    d1 = tf.reshape(\n",
    "        tf.range(4*3),\n",
    "        (4, 3)\n",
    "    )\n",
    "    d2 = tf.range(4)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((d1, d2))\n",
    "    \n",
    "    dataset = (\n",
    "        dataset\n",
    "        .apply(tf.contrib.data.shuffle_and_repeat(buffer_size))\n",
    "        .apply(tf.contrib.data.map_and_batch(map_fn, batch_size, num_parallel_batches))\n",
    "    )\n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "#     return iterator\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "---\n",
      "(array([[3, 4, 5],\n",
      "       [0, 1, 2]], dtype=int32), array([1, 0], dtype=int32))\n",
      "(array([[ 9, 10, 11],\n",
      "       [ 6,  7,  8]], dtype=int32), array([3, 2], dtype=int32))\n",
      "---\n",
      "(array([[0, 1, 2],\n",
      "       [3, 4, 5]], dtype=int32), array([0, 1], dtype=int32))\n",
      "(array([[ 9, 10, 11],\n",
      "       [ 6,  7,  8]], dtype=int32), array([3, 2], dtype=int32))\n",
      "---\n",
      "(array([[0, 1, 2],\n",
      "       [6, 7, 8]], dtype=int32), array([0, 2], dtype=int32))\n",
      "(array([[ 3,  4,  5],\n",
      "       [ 9, 10, 11]], dtype=int32), array([1, 3], dtype=int32))\n",
      "\n",
      "****** next_batch2 ******\n",
      "\n",
      "---\n",
      "(array([[0, 1, 2],\n",
      "       [3, 4, 5]], dtype=int32), array([0, 1], dtype=int32))\n",
      "(array([[ 9, 10, 11],\n",
      "       [ 6,  7,  8]], dtype=int32), array([3, 2], dtype=int32))\n",
      "---\n",
      "(array([[3, 4, 5],\n",
      "       [6, 7, 8]], dtype=int32), array([1, 2], dtype=int32))\n",
      "(array([[ 9, 10, 11],\n",
      "       [ 0,  1,  2]], dtype=int32), array([3, 0], dtype=int32))\n",
      "---\n",
      "(array([[0, 1, 2],\n",
      "       [6, 7, 8]], dtype=int32), array([0, 2], dtype=int32))\n",
      "(array([[ 3,  4,  5],\n",
      "       [ 9, 10, 11]], dtype=int32), array([1, 3], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    next_batch = input_fn()\n",
    "    \n",
    "    for i in range(6):\n",
    "        if i % 2 == 0:\n",
    "            print('---')\n",
    "        print(sess.run(next_batch))\n",
    "        \n",
    "    next_batch2 = input_fn()\n",
    "    \n",
    "    print('\\n****** next_batch2 ******\\n')\n",
    "    for i in range(6):\n",
    "        if i % 2 == 0:\n",
    "            print('---')\n",
    "        print(sess.run(next_batch2))\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((\n",
    "#     tf.random_uniform([4, 10], maxval=100, dtype=tf.int32),  # train data\n",
    "#     tf.random_uniform([4])  # train label\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn():\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((\n",
    "        tf.random_uniform([4, 10], maxval=100, dtype=tf.int32),  # train data\n",
    "        tf.random_uniform([4])  # train label\n",
    "    ))\n",
    "    \n",
    "    \n",
    "    \n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "    return tf.estimator.EstimatorSpec(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpsxbk1xuu\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpsxbk1xuu', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe7f7256240>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x7fe7f612d0d0>) includes params argument, but params are not passed to Estimator.\n"
     ]
    }
   ],
   "source": [
    "estimator = tf.estimator.Estimator(model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(input_fn, steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(os.environ.get('hello_world_env'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.FileHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import importlib\n",
    "import tqdm\n",
    "\n",
    "MODEL_DIR_PREFIX = '/home/shared/cs231n-fashion/model_dir'\n",
    "\n",
    "def write_predictions(probs):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        probs: \n",
    "    \n",
    "    Globals:\n",
    "        FLAGS.test_prediction\n",
    "        FLAGS.pred_threshold\n",
    "    \"\"\"\n",
    "    print(\"Saving test data to: \", FLAGS.test_prediction)\n",
    "    f = open(FLAGS.test_prediction, \"w\")\n",
    "    f.write(\"image_id,label_id\\n\")\n",
    "    img_id = 1\n",
    "\n",
    "    # deal with unified threshold or per class thresholding\n",
    "    thresholds = []\n",
    "    if re.match(\"^\\d+?\\.\\d+?$\", FLAGS.pred_threshold) is None:\n",
    "        print(\"Use per class thresholding.\")\n",
    "        thresholds = pd.read_csv(FLAGS.pred_threshold)['thresholds'].values\n",
    "    else:\n",
    "        th = float(FLAGS.pred_threshold)\n",
    "        thresholds = [th for i in range(228)]\n",
    "\n",
    "    with tqdm(total=NUM_TEST) as progress_bar:\n",
    "        for prob in probs:\n",
    "            labels=\" \".join([\n",
    "                str(i+1)\n",
    "                for i in range(len(prob))\n",
    "                if prob[i] >= thresholds[i]\n",
    "            ])\n",
    "            f.write(\"%d,%s\\n\"%(img_id, labels))\n",
    "            img_id += 1\n",
    "            progress_bar.update(1)\n",
    "    print(\"Processed %d examples. Good Luck! :)\"%(img_id))\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "def predict_with_ensemble(ensemble_label_to_model_meta, run_config, params, test_input_fn):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        ensemble_label_to_model_meta: dict. KV is in the following format:\n",
    "            'label': ('model_file', 'exp_name', weight).\n",
    "            The model_file should export model_fn(features, labels, mode, params).\n",
    "            The exp_name should be the folder name that stores the weights of the model.\n",
    "    \"\"\"\n",
    "    # Validate existence of exp_names.\n",
    "\n",
    "    total_weight = 0.0\n",
    "    agg_probs = None\n",
    "\n",
    "    for label in ensemble_label_to_model_meta:\n",
    "        model_file, exp_name, weight = ensemble_label_to_model_meta[label]\n",
    "        model_dir = os.path.join(MODEL_DIR_PREFIX, exp_name)\n",
    "\n",
    "        # Create model from model_file.\n",
    "        model_fn = importlib.import_module(model_file).model_fn\n",
    "\n",
    "        # Initialize classifier with weights from exp_name.\n",
    "        classifier = tf.estimator.Estimator(\n",
    "            config=run_config,\n",
    "            model_fn=model_fn,\n",
    "            model_dir=model_dir,\n",
    "            params=params\n",
    "        )\n",
    "\n",
    "        # Make predictions.\n",
    "        probs = [pred['probs'] for pred in classifier.predict(test_input_fn)]\n",
    "        \n",
    "        # Aggregate weighted probs.\n",
    "        if agg_probs is None:\n",
    "            agg_probs = weight * probs\n",
    "        else:\n",
    "            agg_probs += weight * probs\n",
    "        total_weight += weight\n",
    "    \n",
    "    agg_probs /= total_weight\n",
    "    write_predictions(agg_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KV is 'label': ('model_file', 'exp_name', weight)\n",
    "ensemble_label_to_model_meta = {\n",
    "    'label1': ['base', 'exp1', 0.2],\n",
    "    'label2': ['base', 'exp2', 0.5],\n",
    "}\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    session_config=tf.ConfigProto(log_device_placement=True),\n",
    "#             save_checkpoints_secs=30*60,\n",
    "#             keep_checkpoint_max=10,\n",
    ")\n",
    "\n",
    "params = {\n",
    "    'learning_rate': FLAGS.learning_rate,\n",
    "    'num_classes': FLAGS.num_classes,\n",
    "    'module_trainable': FLAGS.module_trainable,\n",
    "    'eval_thresholds': [float(i) for i in FLAGS.eval_thresholds.split(';')],\n",
    "    'model_dir': FLAGS.model_dir,\n",
    "    'reg': FLAGS.reg,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = logging.getLogger('tensorflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdout_handler = log.handlers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log.removeHandler(stdout_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ensemble\n",
    "\n",
    "1. Read csv file using pandas\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from: /home/minfa/ensemble_dir/debug_dump_test.csv\n",
      "Reading data from: /home/minfa/ensemble_dir/debug_dump_test2.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "prob_dir = '/home/minfa/ensemble_dir'\n",
    "prob_files = os.listdir(prob_dir)\n",
    "agg_probs = None\n",
    "\n",
    "for prob_file in prob_files:\n",
    "    prob_file_path = os.path.join(prob_dir, prob_file)\n",
    "    print(\"Reading data from: {}\".format(prob_file_path))\n",
    "    \n",
    "    probs = pd.read_csv(prob_file_path)['label_prob'].map(lambda x: np.array([float(v) for v in x.split(' ')])).values\n",
    "    probs = np.array(probs.tolist())\n",
    "    if agg_probs is None:\n",
    "        agg_probs = probs\n",
    "    else:\n",
    "        agg_probs += probs\n",
    "\n",
    "agg_probs /= len(prob_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs231n-fashion2",
   "language": "python",
   "name": "cs231n-fashion2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
